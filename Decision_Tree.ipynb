{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7xO3LLfKop0J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "GdOrinFQos_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.What is a Decision Tree, and how does it work in the context of classification?\n"
      ],
      "metadata": {
        "id": "LIZs0AHhouAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression, but it is most commonly applied in classification problems.\n",
        "\n",
        "It works like a flowchart, where data is split into branches based on certain conditions until a decision (prediction) is made.\n",
        "\n",
        "\n",
        " Structure of a Decision Tree\n",
        "\n",
        "\t1.\tRoot Node – The starting point; represents the entire dataset and the first split based on the most important feature.\n",
        "\t2.\tInternal Nodes – Each node represents a feature test (e.g., “Age > 30?”).\n",
        "\t3.\tBranches – The outcome of the test (e.g., “Yes” or “No”).\n",
        "\t4.\tLeaf Nodes – The final decision (class label in classification).\n",
        "\n",
        "\n",
        " How It Works in Classification\n",
        "\n",
        "\t1.\tFeature Selection (Splitting Criteria)\n",
        "The algorithm selects the feature that best separates the data into different classes. Common criteria include:\n",
        "\n",
        "\t•\tGini Index (used in CART)\n",
        "\t•\tEntropy / Information Gain (used in ID3, C4.5)\n",
        "\n",
        "Example: In a dataset of students, if we want to classify whether they “Pass” or “Fail”, the feature “Hours Studied” might be the most important factor for the first split.\n",
        "\n",
        "\t2.\tRecursive Splitting\n",
        "After the root node split, each branch creates a smaller subset of the dataset. The process is repeated recursively until:\n",
        "\n",
        "\t•\tAll records in a node belong to the same class, or\n",
        "\t•\tThe maximum tree depth / stopping criteria is reached.\n",
        "\n",
        "\n",
        " 3.\tPrediction\n",
        "\n",
        "To classify a new instance, the model traverses the tree from the root node to a leaf by following the decision rules.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Example (Binary Classification)\n",
        "\n",
        "Dataset: Predict if a person will buy a product. Features: Age, Income.\n",
        "\n",
        "\t•\tRoot Node: “Age > 30?”\n",
        "\t•\tIf Yes, check “Income > 50k?”\n",
        "\t•\tIf Yes → Buy\n",
        "\t•\tIf No → Don’t Buy\n",
        "\t•\tIf No → Don’t Buy\n",
        "\n",
        "⸻\n",
        "\n",
        "Advantages\n",
        "\n",
        "\t•\tEasy to interpret and visualize.\n",
        "\t•\tHandles both numerical and categorical data.\n",
        "\t•\tRequires little data preprocessing.\n",
        "\n",
        " Disadvantages\n",
        "\n",
        "\t•\tCan easily overfit (too many splits).\n",
        "\t•\tSensitive to noisy data.\n",
        "\t•\tGreedy algorithm (locally optimal, not always globally optimal)."
      ],
      "metadata": {
        "id": "Mw77Abtho8cZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "q8GMGofgqHYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️. Impurity in Decision Trees\n",
        "\n",
        "In a Decision Tree, the goal of each split is to make the child nodes as pure as possible.\n",
        "\n",
        "\t•\tA pure node contains samples from only one class.\n",
        "\t•\tAn impure node contains a mix of classes.\n",
        "\n",
        "To measure impurity, we use Gini Impurity and Entropy.\n",
        "\n",
        "\n",
        "\n",
        "2️.Gini Impurity\n",
        "\n",
        "\t•\tDefinition:\n",
        "Probability that a randomly chosen sample from a node would be misclassified if labeled randomly according to the class distribution in that node.\n",
        "\n",
        "\t•\tFormula:\n",
        "\n",
        "\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "\n",
        "Where:\n",
        "\n",
        "\t•\tC = number of classes\n",
        "\t•\tp_i = proportion of samples of class i in the node\n",
        "\t•\tRange:\n",
        "\t•\t0 → node is completely pure (all samples same class)\n",
        "\t•\tMaximum → node is maximally impure (evenly mixed classes)\n",
        "\t•\tExample:\n",
        "Node has 70% Class A and 30% Class B:\n",
        "Gini = 1 - (0.7^2 + 0.3^2) = 0.42\n",
        "\n",
        "\n",
        "\n",
        "3️. Entropy (Information Gain)\n",
        "\n",
        "\t•\tDefinition:\n",
        "\n",
        "Measures the uncertainty or disorder in a node.\n",
        "\n",
        "\t•\tLower entropy → node is more pure\n",
        "\t•\tHigher entropy → node is more mixed\n",
        "\t•\tFormula:\n",
        "\n",
        "Entropy = - \\sum_{i=1}^{C} p_i \\cdot \\log_2(p_i)\n",
        "\n",
        "\t•\tRange:\n",
        "\t•\t0 → node is pure\n",
        "\t•\t1 (binary case) → maximum disorder\n",
        "\t•\tExample: Node has 70% Class A, 30% Class B:\n",
        "    Entropy = -(0.7 \\cdot \\log_2 0.7 + 0.3 \\cdot \\log_2 0.3) \\approx 0.881\n",
        "\n",
        "\n",
        "\n",
        "4️.Impact on Decision Tree Splits\n",
        "\n",
        "\t•\tDecision Tree chooses the best feature to split based on reducing impurity.\n",
        "\n",
        "How each measure works:\n",
        "\n",
        "\t1.\tGini Impurity:\n",
        "\t•\tChoose the feature that minimizes the weighted Gini of child nodes after split.\n",
        "\t2.\tEntropy:\n",
        "\t•\tChoose the feature that maximizes Information Gain = reduction in entropy.\n",
        "\n",
        "Goal: After each split, nodes are more “pure” → easier to classify new samples."
      ],
      "metadata": {
        "id": "It87llx7qMT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each"
      ],
      "metadata": {
        "id": "RyZYFidarv3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees are prone to overfitting, especially when grown to full depth. Pruning techniques help improve generalization by simplifying the tree. They fall into two broad categories:\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "What it is: Stop growing the tree before it perfectly fits the training data. You set criteria to halt splitting when further splits are unlikely to yield significant gains.\n",
        "\n",
        "Common criteria:\n",
        "\n",
        "Maximum depth (limit how deep the tree can grow)\n",
        "\n",
        "Minimum samples per leaf (stop splitting if a node has too few samples)\n",
        "\n",
        "Minimum impurity decrease (require a minimum gain to justify a split)\n",
        "\n",
        "Validation-based stopping (stop when performance on a hold-out set stops improving)\n",
        "\n",
        "How it works in practice:\n",
        "\n",
        "As you build the tree top-down, you check the stopping criteria at each potential split and refrain from adding new nodes if the criterion isn’t met.\n",
        "\n",
        "Practical advantage\n",
        "\n",
        "Faster and simpler models with built-in regularization: Pre-pruning prevents overfitting by design, often resulting in smaller trees that train quickly and generalize better without needing a separate pruning phase.\n",
        "\n",
        "Post-Pruning (Cost-Complexity Pruning, Full-prune)\n",
        "What it is: Grow the full tree (often to the point of overfitting) and then prune back branches that do not provide enough predictive power.\n",
        "\n",
        "Common approaches:\n",
        "\n",
        "Cost-Complexity Pruning (also known as weakest link pruning): balance tree accuracy against tree size using a complexity parameter α.\n",
        "\n",
        "Reduced-error pruning: remove subtrees that do not improve validation set performance.\n",
        "\n",
        "How it works in practice:\n",
        " After fully growing the tree, evaluate the impact of removing each subtree and iteratively prune the branches that yield the best improvement (or least degradation) on a separate validation set.\n",
        "\n",
        "Practical advantage\n",
        "\n",
        "Potentially better performance with optimal simplification: Post-pruning can produce a more accurate model by tailoring the final size of the tree to the data, especially when the initial fully grown tree captured noise. It often achieves a better bias-variance trade-off than a single fixed-depth pre-pruned tree.\n",
        "\n",
        "Quick comparison\n",
        "\n",
        "Growth phase:\n",
        "\n",
        "Pre-pruning: Stop early during growth.\n",
        "\n",
        "Post-pruning: Grow to full depth, then prune.\n",
        "\n",
        "Control signal:\n",
        "\n",
        "Pre-pruning: Simple, resource-efficient criteria applied during building.\n",
        "\n",
        "Post-pruning: Uses validation metrics after full growth to decide what to prune.\n",
        "\n",
        "Risk:\n",
        "\n",
        "Pre-pruning: Risk of underfitting if stopping criteria are too aggressive.\n",
        "\n",
        "Post-pruning: Risk of overfitting during growth, but can yield better final generalization after pruning.\n",
        "\n",
        "Typical use cases:\n",
        "\n",
        "Pre-pruning: When you want fast training and a compact model (e.g., in resource-constrained environments).\n",
        "\n",
        "Post-pruning: When you suspect there is structure in the data that a fully grown tree can reveal but needs simplification to generalize well."
      ],
      "metadata": {
        "id": "yzDTcMf5r1zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ],
      "metadata": {
        "id": "fD_FW0w2sxy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Information Gain?\n",
        "\n",
        "Information Gain (IG) measures how much uncertainty or disorder is reduced by splitting a node using a particular feature.\n",
        "\n",
        "\t•\tIt is based on Entropy, which quantifies the impurity of a node.\n",
        "\t•\tWhen we split a node, we want child nodes to be more “pure” (less mixed classes) than the parent node.\n",
        "\t•\tInformation Gain = Reduction in Entropy after the split.\n",
        "\n",
        "Formula:\n",
        "\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
        "\n",
        "Where:\n",
        "\n",
        "\t•\tS = dataset at the current node\n",
        "\t•\tA = feature we are splitting on\n",
        "\t•\tS_v = subset of S for which feature A has value v\n",
        "\t•\t|S_v|/|S| = proportion of samples in that subset\n",
        "\n",
        "\n",
        "\n",
        "2. Step-by-step Explanation\n",
        "\n",
        "\n",
        "  . Compute Entropy of the parent node → measures the current disorder.\n",
        "\n",
        "\t.\tSplit the dataset based on a feature → create child nodes.\n",
        "\n",
        "\t.\tCompute Entropy of each child node → measure disorder after split.\n",
        "\n",
        "\t.\tWeighted average of child entropies → combine them.\n",
        "\n",
        "\t.\tInformation Gain = Parent Entropy - Weighted Child Entropy\n",
        "\n",
        " A higher Information Gain means the feature splits the data better → child nodes are more pure.\n",
        "\n",
        "\n",
        "\n",
        "3️. Why is Information Gain Important?\n",
        "\n",
        "\t•\tDecision Trees need to decide which feature to split on at each node.\n",
        "\t•\tInformation Gain is the metric used in algorithms like ID3 and C4.5 to choose the best feature.\n",
        "\t•\tFeature with highest Information Gain → chosen for the split.\n",
        "\t•\tThis ensures that each split reduces uncertainty the most, making the tree more accurate and efficient.\n",
        "\n",
        "\n",
        "\n",
        "4️.Example (Simple)\n",
        "\n",
        "\n",
        "Suppose you want to predict if someone will buy a product.\n",
        "\n",
        "\t•\tFeature “Age” splits the dataset into:\n",
        "\t•\tYoung → mostly “No” (Entropy low)\n",
        "\t•\tOld → mostly “Yes” (Entropy low)\n",
        "\t•\tParent node entropy was high (mixed “Yes”/“No”)\n",
        "\t•\tInformation Gain = Parent Entropy – Weighted Child Entropy → high\n",
        "\t•\tSo, “Age” is a good feature to split on."
      ],
      "metadata": {
        "id": "LF6FzPuVs3dP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n"
      ],
      "metadata": {
        "id": "jhXKAhCUuomK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️.Common Real-World Applications of Decision Trees\n",
        "\n",
        "\n",
        "Decision Trees are versatile and widely used in many domains, especially for classification and regression tasks.\n",
        "\n",
        " Some examples:\n",
        "\n",
        "\n",
        "a) Healthcare\n",
        "\n",
        "\t•\tDiagnosing diseases based on patient symptoms and test results.\n",
        "\n",
        "\t•\tExample: Predicting whether a patient has diabetes or heart disease.\n",
        "\n",
        "b) Finance\n",
        "\n",
        "\t•\tCredit scoring and loan approval.\n",
        "\t•\tExample: Predicting if a person is a good credit risk based on income, age, debt, etc.\n",
        "\t•\tFraud detection in banking transactions.\n",
        "\n",
        "c) Marketing\n",
        "\n",
        "\t•\tCustomer segmentation and targeting.\n",
        "\t•\tExample: Predicting which customers are likely to respond to a promotion.\n",
        "\n",
        "d) E-commerce\n",
        "\n",
        "\t•\tRecommendation systems.\n",
        "\t•\tExample: Predicting whether a user will buy a product based on browsing history.\n",
        "\n",
        "e) Manufacturing\n",
        "\n",
        "\t•\tQuality control and defect detection.\n",
        "\t•\tExample: Predicting defective products using machine sensor data.\n",
        "\n",
        "f) Operations & HR\n",
        "\n",
        "\t•\tEmployee attrition prediction.\n",
        "\t•\tExample: Predicting whether an employee is likely to leave the company.\n",
        "\n",
        "\n",
        "\n",
        "2️. Main Advantages of Decision Trees\n",
        "\n",
        "\t1.\tEasy to understand and interpret\n",
        "\t•\tTrees can be visualized as flowcharts; non-technical people can understand them.\n",
        "\t2.\tHandles both numerical and categorical data\n",
        "\t3.\tNo need for feature scaling\n",
        "\t•\tUnlike algorithms like SVM or KNN, Decision Trees don’t require normalization.\n",
        "\t4.\tCan capture non-linear relationships\n",
        "\t•\tSplits allow complex decision boundaries.\n",
        "\t5.\tCan handle missing values (some implementations)\n",
        "\t6.\tFast predictions\n",
        "\t•\tTraversing a tree is computationally simple.\n",
        "\n",
        "\n",
        "\n",
        "3️. Main Limitations of Decision Trees\n",
        "\n",
        "\t1.\tOverfitting\n",
        "\t•\tTrees can become too deep and memorize training data, reducing generalization.\n",
        "\t2.\tSensitive to small data changes\n",
        "\t•\tA small change in the data can lead to a completely different tree.\n",
        "\t3.\tBiased with imbalanced data\n",
        "\t•\tFeatures with many categories may dominate splits.\n",
        "\t4.\tGreedy algorithm\n",
        "\t•\tLocally optimal splits may not lead to globally optimal trees.\n",
        "\t5.\tLess accurate than ensemble methods\n",
        "\t•\tSingle Decision Trees are often outperformed by Random Forests or Gradient Boosting."
      ],
      "metadata": {
        "id": "P3kvp09nutKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "9ZVFDOILvIyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Import Libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier using Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# Print Feature Importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhE7CQGhvhiz",
        "outputId": "b67bb357-fd63-4e8e-f2e2-da993af2ada6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "Ott65QqSv_ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Train fully-grown Decision Tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy (max_depth=3): {accuracy_depth3*100:.2f}%\")\n",
        "print(f\"Accuracy (fully-grown tree): {accuracy_full*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gT5otlKwL6b",
        "outputId": "1e767bf0-0d6b-4ac1-c565-33a917bee579"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 100.00%\n",
            "Accuracy (fully-grown tree): 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "kOgOCQh5wj9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing Dataset\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Print Feature Importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B2NeDzEwsYQ",
        "outputId": "64c7ea18-d683-4a49-9858-1cf7f684f65a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.42\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9.Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "qsoG-KoFxadk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Evaluate the model with best parameters on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the best model: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACIzpy8BxhEd",
        "outputId": "5f84de20-e5f3-476b-9da3-21b6f8ab5f2d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy of the best model: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting"
      ],
      "metadata": {
        "id": "vHftyoCSxxp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handling Missing Values\n",
        "\n",
        "\t•\tImportance: Missing data can introduce bias, reduce accuracy, or prevent the model from learning correctly.\n",
        "\n",
        "\t•\tSteps:\n",
        "\t1.\tIdentify missing values: Check which features contain null values or placeholders like NA.\n",
        "\t2.\tAnalyze patterns: Determine if missingness is random (MCAR), dependent on other variables (MAR), or non-random (MNAR).\n",
        "\t3.\tImpute missing values:\n",
        "\t•\tNumerical features: Use median or mean to replace missing values. Median is preferred if the data has outliers.\n",
        "\n",
        "\t•\tCategorical features: Replace missing values with the mode (most frequent category) or a special label like \"Unknown\" to indicate missingness.\n",
        "\n",
        "\t4.\tDrop features if necessary: If a feature has too many missing values (e.g., >50%), it may be safer to drop it to avoid noise.\n",
        "\n",
        "\t•\tBusiness Impact: Proper handling of missing values ensures that the predictions are accurate and trustworthy, which is critical in healthcare.\n",
        "\n",
        "\n",
        "\n",
        "2. Encoding Categorical Features\n",
        "\n",
        "\t•\tWhy: Machine learning algorithms, including Decision Trees in libraries like scikit-learn, require numerical inputs.\n",
        "\n",
        "\t•\tTechniques:\n",
        "\n",
        "\t1.\tLabel Encoding: Assigns a unique integer to each category. Suitable for ordinal variables (e.g., low, medium, high risk).\n",
        "\n",
        "\t2.\tOne-Hot Encoding: Creates a binary column for each category. Best for nominal variables (e.g., gender, blood type) without any natural order.\n",
        "\n",
        "\t•\tCombining features: After encoding categorical features, they are merged with numerical features to form the final dataset.\n",
        "\n",
        "\n",
        "\n",
        "3. Training a Decision Tree Model\n",
        "\n",
        "•\tWhy Decision Tree:\n",
        "\n",
        "\t  •\tIntuitive and interpretable, which is crucial in healthcare for explaining predictions.\n",
        "\t  •\tCan handle both numerical and categorical features.\n",
        "\t  •\tCaptures non-linear relationships between features and target disease outcome.\n",
        "\n",
        "\t•\tSteps:\n",
        "\n",
        "\t1.\tSplit data into training and testing sets to evaluate model performance on unseen data.\n",
        "\t2.\tInitialize a DecisionTreeClassifier.\n",
        "\t3.\tFit the model on the training data to learn decision rules based on features.\n",
        "\n",
        "\t•\tOutcome: The tree splits the data at each node based on features that best reduce impurity (using Gini or Entropy).\n",
        "\n",
        "\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "\t•\tWhy: Default trees often overfit, especially on complex datasets. Tuning parameters ensures better generalization.\n",
        "\n",
        "•\tKey hyperparameters:\n",
        "\t•\tmax_depth: Maximum depth of the tree to control complexity.\n",
        "\n",
        "\t•\tmin_samples_split: Minimum samples required to split a node.\n",
        "\n",
        "\t•\tmin_samples_leaf: Minimum samples in a leaf node.\n",
        "\n",
        "\t•\tcriterion: “gini” or “entropy” to measure impurity.\n",
        "\n",
        "\t•\tTechnique: Use GridSearchCV to systematically search for the combination of hyperparameters that maximizes cross-validated accuracy.\n",
        "\n",
        "\t•\tBusiness Impact: Tuning ensures that the model avoids overfitting, producing reliable predictions for real patients.\n",
        "\n",
        "\n",
        "\n",
        "5. Model Evaluation\n",
        "\n",
        "•\tMetrics:\n",
        "\t•\tAccuracy: Overall correctness of predictions.\n",
        "\n",
        "\t•\tPrecision: How many predicted positives are actually positive (important to avoid unnecessary treatment).\n",
        "\t•\tRecall (Sensitivity): How many actual positives were correctly identified (critical in healthcare to avoid missed diagnoses).\n",
        "\t•\tF1-score: Balance between precision and recall.\n",
        "\t•\tROC-AUC: Measures discriminative ability of the model.\n",
        "•\tImportance in healthcare: Prioritizing high recall ensures disease cases are not missed, even if it means some false positives.\n",
        "\n",
        "\n",
        "\n",
        "6. Business Value\n",
        "\n",
        "\t•\tEarly Detection: Identify high-risk patients before severe symptoms appear.\n",
        "\n",
        "\t•\tResource Optimization: Prioritize patients for expensive tests or specialist consultations.\n",
        "\n",
        "\t•\tCost Reduction: Reduce unnecessary tests for low-risk patients.\n",
        "\n",
        "\t•\tImproved Patient Outcomes: Early and accurate diagnosis leads to better treatment plans and outcomes.\n",
        "  \n",
        "\t•\tData-Driven Decision Making: Hospitals can allocate staff, equipment, and budget more effectively using predictive insights."
      ],
      "metadata": {
        "id": "SERTTWtuyHQr"
      }
    }
  ]
}